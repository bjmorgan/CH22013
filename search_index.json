[["index.html", "CH22013 Lecture Notes About", " CH22013 Lecture Notes Benjamin J. Morgan 2025-03-21 About These notes accompany the 2025 CH22013 lecture course on Monte Carlo methods. "],["introduction-to-monte-carlo-methods.html", "Lecture 1 Introduction to Monte Carlo Methods 1.1 Introduction to Averaging &amp; Sampling 1.2 Essentials of Random Sampling 1.3 Core Probability Concepts for Monte Carlo 1.4 Simple Demonstrations of Monte Carlo Methods 1.5 Comparison with Other Simulation Techniques 1.6 Error Analysis in Monte Carlo Simulations 1.7 Convergence Criteria 1.8 Summary", " Lecture 1 Introduction to Monte Carlo Methods 1.1 Introduction to Averaging &amp; Sampling In computational chemistry, we seek to calculate observable properties of molecular systems. These properties—such as energy, density, or heat capacity—represent the average behaviour of countless molecules interacting over time. The challenge lies in computing these averages efficiently and accurately. 1.1.1 The Computational Chemistry Challenge How do we calculate measurable properties of chemical systems? The properties we care about in chemistry—energy, structure, heat capacity—are statistical averages across many possible molecular arrangements. At finite temperature, molecules constantly move and explore different configurations, making the calculation of properties more complex than simply evaluating a single structure. 1.1.2 Potential Energy and Configurations The starting point for most computational chemistry methods is the potential energy function, denoted as \\(U(\\mathbf{r})\\). This function gives us the energy for any particular arrangement of atoms (configuration) in our system. The variable \\(\\mathbf{r}\\) represents the coordinates of all atoms. For a system at finite temperature, molecules constantly move and explore many different configurations. Some configurations are more likely than others, with lower-energy states being favoured. The probability of finding a system in a particular configuration follows the Boltzmann distribution: \\[P(\\mathbf{r}) \\propto \\exp(-U(\\mathbf{r})/kT)\\] Where \\(k\\) is Boltzmann’s constant and \\(T\\) is the temperature. 1.1.3 The Averaging Problem To calculate a thermodynamic property \\(A\\), we need to compute its average across all possible configurations, with each configuration weighted by its probability: \\[\\langle A \\rangle = \\sum A(\\mathbf{r}) \\times P(\\mathbf{r})\\] Where the sum runs over all possible configurations. This seemingly simple equation presents an enormous computational challenge. Consider a modest molecular system with just 20 atoms. If each atom could occupy only 10 possible positions (a gross simplification), we would need to evaluate \\(10^{20}\\) different configurations. This number exceeds the count of stars in the observable universe! Clearly, direct summation is impossible for any realistic chemical system. 1.1.4 The General Idea of Sampling Rather than attempting to evaluate every possible configuration, we can estimate the average by examining only a representative sample of configurations. This approach mirrors how we might estimate the average height of people in Britain—we measure a carefully chosen subset rather than the entire population. For many mathematical problems, uniform random sampling works well. Estimating the area of irregular shapes or performing numerical integration can be effectively done with uniform sampling techniques. 1.1.5 The Special Challenge of Chemical Systems For chemistry, there’s a critical distinction—some configurations are much more likely than others. The Boltzmann distribution tells us that lower-energy configurations are exponentially more likely than higher-energy ones. This means we need a weighted sample that favors important (lower energy) configurations. 1.1.6 A Brief Look at MD Sampling In Molecular Dynamics (MD), we simulate the physical motion of atoms over time, which naturally visits configurations according to their probability. The average of a property \\(A\\) can then be estimated by: \\[\\langle A \\rangle \\approx \\frac{1}{M} \\sum_{i=1}^{M} A(\\mathbf{r}_i)\\] Where \\(M\\) is the number of time steps (frames) in our trajectory. However, MD faces significant challenges. Energy barriers between different states can trap the system in local energy minima for extended periods, preventing adequate sampling of all relevant configurations. Some configurations may be rarely visited, leading to incomplete sampling. 1.2 Essentials of Random Sampling Monte Carlo methods employ random numbers to solve deterministic mathematical problems. This seemingly counterintuitive approach is powerful because it allows us to estimate properties that would be impossible to calculate directly. 1.2.1 Basic Concept The fundamental idea behind Monte Carlo methods is using random sampling to solve problems that might otherwise be intractable. By drawing random samples and evaluating the function of interest at these points, we can estimate averages, integrals, and other mathematical quantities. 1.2.2 Simple Uniform Sampling In its most basic form, Monte Carlo sampling involves drawing random points uniformly within a defined region. Each point has an equal probability of being selected, meaning we sample the space without any bias toward particular regions. This approach forms the foundation for our initial examples and is particularly effective for many mathematical problems where all regions of the domain are equally important. 1.2.3 Accessing Random Numbers in Practice Most programming languages provide built-in functions that generate random numbers uniformly distributed between 0 and 1. For example: - Python: random.random() - MATLAB: rand() - C++: rand() / RAND_MAX An important practical consideration is setting the random number generator’s “seed”—a value that initializes the sequence. Using the same seed ensures reproducible results, which is essential for scientific work and debugging. 1.3 Core Probability Concepts for Monte Carlo To understand why Monte Carlo methods work, we need a few key concepts from probability theory. 1.3.1 Expected Values The expected value of a property \\(A\\), denoted \\(E[A]\\) or \\(\\langle A \\rangle\\), represents the long-term average value we would obtain if we could measure \\(A\\) over all possible configurations. Mathematically: \\[E[A] = \\sum A(\\mathbf{r}) \\times P(\\mathbf{r})\\] This is precisely the thermodynamic average we seek to calculate. The challenge lies in estimating this value efficiently. In simple terms, the expected value is “the value we would get with infinite samples”—it’s our target for estimation. 1.3.2 The Law of Large Numbers The Law of Large Numbers provides the mathematical foundation for Monte Carlo methods. It states that as the sample size increases, the average of the samples approaches the true expected value. For example, if we flip a fair coin many times, the proportion of heads will converge to 0.5 as the number of flips increases. With 10 flips, we might see 7 heads (0.7), but with 10,000 flips, the proportion will be much closer to 0.5. In Monte Carlo simulations, this law guarantees that with sufficient samples, our estimate will approach the true average. The convergence can be visualized by plotting the running average against the number of samples. 1.3.3 Uncertainty in Monte Carlo Estimates Every Monte Carlo estimate comes with statistical uncertainty. As the number of samples (\\(N\\)) increases, the uncertainty decreases proportionally to \\(1/\\sqrt{N}\\). This means that to halve the uncertainty, we need four times as many samples. For students applying Monte Carlo methods, a simple approach to estimating uncertainty involves: 1. Breaking the samples into several batches 2. Computing the average for each batch 3. Calculating the standard deviation of these batch averages This gives a reasonable estimate of the statistical error in our results. 1.4 Simple Demonstrations of Monte Carlo Methods Let’s examine two classic examples that illustrate the power of Monte Carlo methods. 1.4.1 Estimating π by Monte Carlo Integration One of the simplest demonstrations of Monte Carlo methods is estimating the value of \\(\\pi\\). Consider a circle with radius 1 inscribed inside a square with side length 2. The area of the circle is \\(\\pi\\), while the area of the square is 4. Therefore, the ratio of these areas is \\(\\pi/4\\). We can estimate this ratio by randomly placing points within the square and counting what fraction fall inside the circle. A point \\((x,y)\\) falls inside the circle if \\(x^2 + y^2 \\leq 1\\). The procedure works as follows: 1. Generate \\(N\\) random points with coordinates \\((x,y)\\), where \\(x\\) and \\(y\\) are uniformly distributed between -1 and 1 2. Count how many points \\(M\\) satisfy \\(x^2 + y^2 \\leq 1\\) 3. The estimate for \\(\\pi\\) is given by: \\(\\pi \\approx 4 \\times M/N\\) This method converges slowly—doubling the precision requires quadrupling the number of points—but it elegantly demonstrates the Monte Carlo approach. A simple implementation might look like: import random import matplotlib.pyplot as plt import numpy as np def estimate_pi(num_points): &quot;&quot;&quot; Estimate the value of pi using Monte Carlo method. Args: num_points: Number of random points to generate Returns: Estimate of pi &quot;&quot;&quot; points_in_circle = 0 points_x = [] points_y = [] circle_x = [] circle_y = [] square_x = [] square_y = [] for _ in range(num_points): x = random.uniform(-1, 1) y = random.uniform(-1, 1) points_x.append(x) points_y.append(y) if x**2 + y**2 &lt;= 1: points_in_circle += 1 circle_x.append(x) circle_y.append(y) else: square_x.append(x) square_y.append(y) # Plot the points plt.figure(figsize=(8, 8)) plt.scatter(circle_x, circle_y, color=&#39;blue&#39;, s=1) plt.scatter(square_x, square_y, color=&#39;red&#39;, s=1) plt.axis(&#39;equal&#39;) plt.title(f&#39;Estimating π: {4 * points_in_circle / num_points:.6f}&#39;) plt.savefig(&#39;pi_estimation.png&#39;, dpi=300) return 4 * points_in_circle / num_points When we visualize this process, we can color points based on whether they fall inside the circle (e.g., blue) or outside (e.g., red). As the number of points increases, our estimate becomes more accurate. Estimating π with Monte Carlo We can also analyze how the error in our estimation decreases with sample size, demonstrating the \\(1/\\sqrt{N}\\) scaling of uncertainty. 1.4.2 Function Averaging / Numerical Integration Our second example involves calculating the average value of a function over a specific interval. Consider finding the average value of \\(\\sin^2(x)\\) over the interval \\([0,\\pi]\\). Mathematically, this average is defined as: \\[\\text{Average value} = \\frac{1}{\\pi} \\times \\int_0^{\\pi} \\sin^2(x) dx\\] The analytical solution to this integral is \\(1/2\\). Using Monte Carlo methods, we can estimate this average by: 1. Generating uniform random \\(x\\)-values between 0 and \\(\\pi\\) 2. Calculating \\(\\sin^2(x)\\) for each point 3. Averaging the results import random import math import matplotlib.pyplot as plt import numpy as np def average_sin_squared(num_points): &quot;&quot;&quot; Estimate the average value of sin^2(x) over [0,π] using Monte Carlo. Args: num_points: Number of random points to generate Returns: Estimated average &quot;&quot;&quot; total = 0 x_values = [] y_values = [] for _ in range(num_points): x = random.uniform(0, math.pi) y = math.sin(x)**2 total += y x_values.append(x) y_values.append(y) # Plot the function and sample points plt.figure(figsize=(10, 6)) x = np.linspace(0, math.pi, 1000) plt.plot(x, np.sin(x)**2, &#39;b-&#39;, label=&#39;$\\sin^2(x)$&#39;) plt.scatter(x_values[:100], y_values[:100], color=&#39;red&#39;, alpha=0.5, s=20, label=&#39;Sample points&#39;) plt.axhline(y=0.5, color=&#39;g&#39;, linestyle=&#39;--&#39;, label=&#39;True average (0.5)&#39;) plt.axhline(y=total/num_points, color=&#39;r&#39;, linestyle=&#39;--&#39;, label=f&#39;MC estimate ({total/num_points:.4f})&#39;) plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;$\\sin^2(x)$&#39;) plt.legend() plt.title(f&#39;Monte Carlo Integration: Average of $\\sin^2(x)$ over [0,π]&#39;) plt.savefig(&#39;sin_squared_average.png&#39;, dpi=300) return total / num_points This example directly parallels the calculation of thermodynamic averages in chemistry. Both involve computing the average of a function weighted by some probability distribution. The only difference is that in this simple case, our probability distribution is uniform, whereas in chemical systems, we use the Boltzmann distribution. Estimating the average of sin²(x) By plotting the running average against the number of samples, we can observe convergence toward the true value of 1/2. 1.5 Comparison with Other Simulation Techniques Monte Carlo methods offer distinct advantages and disadvantages compared to other approaches in computational chemistry. 1.5.1 Monte Carlo vs. Molecular Dynamics While both methods sample configuration space, they differ in fundamental ways: Molecular Dynamics: - Follows physically realistic trajectories - Provides time-dependent information - Sample configurations are correlated in time - May struggle with rare events or high energy barriers Monte Carlo: - Makes non-physical “jumps” between configurations - Cannot directly provide time-dependent properties - Can be designed to reduce correlation between samples - Often better at sampling rare events or crossing barriers The choice between MC and MD depends on the specific problem. MD excels at studying dynamic processes, while MC often works better for equilibrium properties, especially in systems with significant energy barriers. 1.5.2 Deterministic Numerical Methods Traditional numerical methods, such as numerical integration using the trapezoidal rule or Simpson’s rule, offer alternatives to Monte Carlo for some problems. These methods: - Provide systematic error reduction - Work well in low dimensions - Become impractical in high-dimensional problems due to the “curse of dimensionality” Monte Carlo methods scale more favorably with dimension, making them essential for the high-dimensional spaces encountered in chemical systems. 1.6 Error Analysis in Monte Carlo Simulations Evaluating the accuracy of Monte Carlo results requires understanding both statistical and systematic errors. 1.6.1 Statistical Errors Statistical errors arise from the finite number of samples and follow well-understood principles: - The error decreases as \\(1/\\sqrt{N}\\) with sample size \\(N\\) - Confidence intervals can be calculated using the standard error of the mean - Correlated samples reduce the effective sample size For practical error estimation, the batch averaging method works well: 1. Divide the simulation into blocks of equal size 2. Calculate the average within each block 3. Compute the standard deviation of the block averages 4. The standard error equals this standard deviation divided by \\(\\sqrt{\\text{number of blocks}}\\) This approach accounts for correlation between consecutive samples. 1.6.2 Practical Error Estimation When reporting Monte Carlo results, it’s essential to include an estimate of uncertainty. For example, rather than stating \\(\\pi \\approx 3.14\\), we might report \\(\\pi \\approx 3.14 \\pm 0.01\\), where the second number represents our statistical uncertainty. 1.6.3 Sources of Systematic Errors Unlike statistical errors, systematic errors don’t decrease with more samples. They result from problems in the simulation setup or implementation: - Insufficient sampling of important regions - Biased random number generation - Programming errors Identifying systematic errors often requires running multiple simulations with different parameters or starting conditions. 1.7 Convergence Criteria A critical question in any Monte Carlo simulation is: “How do we know when we have enough samples?” 1.7.1 Running Averages One practical technique involves plotting the running average of your property of interest against the number of samples. As the simulation converges, this average should stabilize around the true value. def plot_running_average(num_points=10000): &quot;&quot;&quot; Plot the running average of the π estimate as the number of points increases. Args: num_points: Total number of points to simulate &quot;&quot;&quot; points_in_circle = 0 estimates = [] for i in range(1, num_points + 1): x = random.uniform(-1, 1) y = random.uniform(-1, 1) if x**2 + y**2 &lt;= 1: points_in_circle += 1 # Calculate and store the current estimate if i % 100 == 0: # Store every 100th estimate to keep plot manageable estimates.append(4 * points_in_circle / i) # Plot running average plt.figure(figsize=(10, 6)) plt.plot(range(100, num_points + 1, 100), estimates, &#39;b-&#39;) plt.axhline(y=math.pi, color=&#39;r&#39;, linestyle=&#39;--&#39;, label=&#39;True value (π)&#39;) plt.xlabel(&#39;Number of samples&#39;) plt.ylabel(&#39;Estimate of π&#39;) plt.title(&#39;Running Average of π Estimate&#39;) plt.legend() plt.savefig(&#39;pi_running_average.png&#39;, dpi=300) Running average of π estimate 1.7.2 Statistical Tests More formal approaches include statistical tests that evaluate whether additional samples are likely to change the result significantly. However, for introductory purposes, visual inspection of running averages often suffices. 1.7.3 Balancing Accuracy and Computational Cost There’s always a trade-off between accuracy and computational expense. The \\(1/\\sqrt{N}\\) scaling of error means that to gain another decimal place of precision, we need 100 times more samples. At some point, the additional computational cost may not justify the marginal improvement in accuracy. 1.8 Summary Monte Carlo methods provide powerful tools for estimating averages in complex systems. Starting from the fundamental challenge of computing thermodynamic averages, we’ve seen how random sampling offers a practical solution. We’ve explored: - The basic principles of random sampling - Core probability concepts that explain why MC works - Simple demonstrations that illustrate MC techniques - Comparisons with alternative approaches - Methods for analyzing errors and determining convergence In the next lecture, we’ll extend these concepts to chemical systems, introducing the challenge of non-uniform distributions in chemistry and exploring the Metropolis algorithm as a solution. "],["monte-carlo-applied-to-chemical-systems.html", "Lecture 2 Monte Carlo Applied to Chemical Systems 2.1 From Uniform to Non-Uniform Sampling 2.2 Probability Distributions in Chemistry 2.3 The Curse of Dimensionality 2.4 Markov Chain Monte Carlo Fundamentals 2.5 Detailed Balance - The Key Principle 2.6 The Metropolis Algorithm 2.7 Implementing Metropolis MC for Chemical Systems 2.8 Chemical Example: Conformational Sampling of Butane 2.9 Summary", " Lecture 2 Monte Carlo Applied to Chemical Systems 2.1 From Uniform to Non-Uniform Sampling In our previous lecture, we explored Monte Carlo methods using uniform sampling—where each possible state or configuration has an equal probability of being selected. We saw how this approach works for mathematical problems like estimating π or calculating definite integrals. 2.1.1 The Shift to Chemical Systems When we move from mathematical problems to chemical systems, we encounter a fundamental difference: not all molecular configurations are equally likely. In chemistry, the probability of observing a particular configuration depends on its energy according to the Boltzmann distribution: \\[P(\\mathbf{r}) \\propto \\exp(-U(\\mathbf{r})/kT)\\] This means that low-energy configurations are exponentially more likely than high-energy ones. At room temperature, a configuration that is just 6 kJ/mol higher in energy is approximately 10 times less probable. 2.1.2 From Simple to Weighted Averages This non-uniform probability distribution changes how we must calculate averages. Instead of a simple arithmetic mean, we need a weighted average: Uniform sampling (Lecture 1): \\(\\langle A \\rangle \\approx \\frac{1}{N} \\times \\sum_{i=1}^{N} A(\\mathbf{r}_i)\\) Weighted sampling (Chemical systems): \\(\\langle A \\rangle \\approx \\frac{\\sum_{i=1}^{N} A(\\mathbf{r}_i) \\times w(\\mathbf{r}_i)}{\\sum_{i=1}^{N} w(\\mathbf{r}_i)}\\) Where \\(w(\\mathbf{r}_i)\\) represents the statistical weight of each configuration, proportional to its Boltzmann factor \\(\\exp(-U(\\mathbf{r}_i)/kT)\\). The challenge becomes: how do we generate samples with the correct Boltzmann weighting? 2.2 Probability Distributions in Chemistry Before addressing this challenge, let’s examine probability distributions more carefully, as they play a central role in computational chemistry. 2.2.1 What is a Probability Distribution? A probability distribution is a function that describes the relative likelihood of different outcomes in a random process. For discrete systems, it gives the probability of each possible state. For continuous systems (like molecular configurations), it gives the probability density. In chemistry, several distributions are particularly important: - The Boltzmann distribution for energies and configurations - Maxwell-Boltzmann distribution for molecular velocities - Radial distribution functions for atomic distances 2.2.2 The Boltzmann Distribution in Detail The Boltzmann distribution determines the probability of finding a system in a particular configuration: \\[P(\\mathbf{r}) = C \\times \\exp(-U(\\mathbf{r})/kT)\\] Where: - \\(U(\\mathbf{r})\\) is the potential energy of configuration \\(\\mathbf{r}\\) - \\(k\\) is Boltzmann’s constant - \\(T\\) is the temperature - \\(C\\) is a normalization constant that ensures all probabilities sum to 1 The physical meaning is that systems spend more time in lower-energy states. The temperature determines how strongly the system favors low-energy states: - At high temperatures, the distribution becomes more uniform - At low temperatures, the system is increasingly restricted to the lowest energy states 2.2.3 The Normalization Problem The normalization constant \\(C\\) must ensure that the total probability across all possible configurations equals 1: \\[C = \\frac{1}{\\sum \\exp(-U(\\mathbf{r})/kT)}\\] This normalization constant has a special name in statistical mechanics: the “partition function” (often denoted as \\(Z\\)). You’ll encounter this important concept again in your later studies of physical chemistry and thermodynamics. The partition function presents a challenge—calculating it requires summing over the same large number of configurations that we were trying to avoid in the first place. This creates a practical dilemma: to calculate the correct probabilities for our weighted average, we need to know the partition function, but calculating it requires evaluating all possible states. Monte Carlo methods offer a solution to this dilemma. 2.3 The Curse of Dimensionality Before discussing that solution, we need to understand another challenge that makes chemical systems difficult: the “curse of dimensionality.” 2.3.1 The Problem of High Dimensions As the dimensionality of a problem increases, the volume of the configuration space grows exponentially. This has implications for sampling efficiency. Consider the volume ratio of a hypersphere inscribed within a hypercube: - In 2D: A circle within a square occupies \\(\\pi/4 \\approx 78.5\\%\\) of the square’s area - In 3D: A sphere within a cube occupies \\(4\\pi/3 \\times (0.5)^3 \\approx 52.4\\%\\) of the cube’s volume - In 10D: The hypersphere occupies only about 0.25% of the hypercube’s hypervolume - In 100D: The ratio becomes very small (approximately \\(10^{-70}\\)) 2.3.2 Implications for Molecular Systems For molecular systems, the dimensionality is determined by the number of degrees of freedom. A system with \\(N\\) atoms typically has \\(3N-6\\) internal degrees of freedom (3N coordinates minus 3 for overall translation and 3 for rotation). Even a small protein with 1000 atoms has approximately 3000 dimensions in its configuration space. If we were to use uniform sampling in this space, almost all random configurations would have negligible Boltzmann weight (likely corresponding to configurations with severe atomic overlaps and very high energies). This means that uniform sampling becomes inefficient as system size increases. The majority of randomly generated configurations would contribute little to our thermodynamic averages. 2.3.3 The Double Challenge We now face two interconnected challenges: 1. We don’t know the exact Boltzmann distribution without calculating the partition function 2. Even if we did, uniform sampling would be inefficient in high dimensions This is where Markov Chain Monte Carlo (MCMC) methods become useful. 2.4 Markov Chain Monte Carlo Fundamentals MCMC methods provide a solution to both challenges by generating a sequence of samples that follow the desired probability distribution, without requiring knowledge of the normalization factor. 2.4.1 The MCMC Concept The idea behind MCMC is to construct a “chain” of configurations, where each new configuration is generated based only on the current one. This sequence is constructed so that the long-term visitation frequency of each state matches the desired probability distribution. Rather than generating independent random samples, we create a “random walk” through configuration space that naturally spends the appropriate amount of time in each region. 2.4.2 Markov Chains A Markov chain is a mathematical system that transitions from one state to another according to certain probabilistic rules. The defining property is that the next state depends only on the current state, not on the sequence of states that preceded it. A Markov chain is characterized by: - A set of possible states - A transition matrix \\(P\\), where \\(P(i\\rightarrow j)\\) gives the probability of moving from state \\(i\\) to state \\(j\\) - The “Markov property”: the future depends only on the present, not the past Over time, many Markov chains converge to a unique “stationary distribution,” where the probability of finding the system in each state no longer changes. 2.4.3 An Illustrative Example Consider a simple Markov chain representing a random walk on a line: - States: Positions 1, 2, 3, …, N - Transition rules: From position \\(i\\), move to \\(i+1\\) with probability 0.5, or to \\(i-1\\) with probability 0.5 (with appropriate adjustments at the boundaries) If we run this Markov chain for a long time, it will spend equal time at each position—its stationary distribution is uniform. The key insight for MCMC methods is that we can design the transition rules to achieve any desired stationary distribution, including the Boltzmann distribution. 2.5 Detailed Balance - The Key Principle To ensure that our Markov chain converges to the correct Boltzmann distribution, we need to satisfy a condition known as “detailed balance.” 2.5.1 Equilibrium in Markov Chains At equilibrium, the population of each state remains constant over time. This means that the total probability flow into a state must equal the total flow out of that state. Mathematically, if \\(\\pi(i)\\) represents the equilibrium probability of state \\(i\\), then: \\[\\sum_i \\pi(i)P(i\\rightarrow j) = \\pi(j)\\] This is sometimes called the “global balance” condition. 2.5.2 The Detailed Balance Condition Detailed balance is a stronger condition that ensures global balance. It requires that for each pair of states \\(i\\) and \\(j\\), the flow from \\(i\\) to \\(j\\) exactly balances the flow from \\(j\\) to \\(i\\): \\[\\pi(i)P(i\\rightarrow j) = \\pi(j)P(j\\rightarrow i)\\] In other words: at equilibrium, the frequency of transitions \\(i\\rightarrow j\\) equals the frequency of transitions \\(j\\rightarrow i\\). Detailed balance is sufficient (though not necessary) to ensure that \\(\\pi\\) is the stationary distribution of the Markov chain. It provides a way to design Markov chains with a specific target distribution. 2.5.3 Applying Detailed Balance to the Boltzmann Distribution For chemical systems, we want \\(\\pi(i)\\) to be the Boltzmann distribution: \\[\\pi(i) \\propto \\exp(-U(i)/kT)\\] Substituting this into the detailed balance equation: \\[\\exp(-U(i)/kT)P(i\\rightarrow j) = \\exp(-U(j)/kT)P(j\\rightarrow i)\\] Rearranging: \\[\\frac{P(i\\rightarrow j)}{P(j\\rightarrow i)} = \\exp\\left(-\\frac{U(j)-U(i)}{kT}\\right)\\] This equation tells us how to design the transition probabilities to ensure that our Markov chain samples from the Boltzmann distribution. 2.6 The Metropolis Algorithm The Metropolis algorithm, developed in the 1950s, provides a way to satisfy detailed balance and generate samples from the Boltzmann distribution. 2.6.1 Separating Transition Probabilities The Metropolis approach separates the transition probability \\(P(i\\rightarrow j)\\) into two components: \\[P(i\\rightarrow j) = \\alpha(i\\rightarrow j) \\times \\text{acc}(i\\rightarrow j)\\] Where: - \\(\\alpha(i\\rightarrow j)\\) is the probability of proposing a move from \\(i\\) to \\(j\\) - \\(\\text{acc}(i\\rightarrow j)\\) is the probability of accepting the proposed move 2.6.2 Satisfying Detailed Balance Substituting this into our detailed balance equation: \\[\\exp(-U(i)/kT)\\alpha(i\\rightarrow j)\\text{acc}(i\\rightarrow j) = \\exp(-U(j)/kT)\\alpha(j\\rightarrow i)\\text{acc}(j\\rightarrow i)\\] Rearranging to isolate the ratio of acceptance probabilities: \\[\\frac{\\text{acc}(i\\rightarrow j)}{\\text{acc}(j\\rightarrow i)} = \\frac{\\exp(-U(j)/kT)\\alpha(j\\rightarrow i)}{\\exp(-U(i)/kT)\\alpha(i\\rightarrow j)}\\] 2.6.3 The Metropolis Choice The Metropolis algorithm makes a specific choice for the acceptance probabilities that satisfies this equation. First, it typically uses symmetric proposal probabilities: \\[\\alpha(i\\rightarrow j) = \\alpha(j\\rightarrow i)\\] With this simplification, the detailed balance condition becomes: \\[\\frac{\\text{acc}(i\\rightarrow j)}{\\text{acc}(j\\rightarrow i)} = \\exp\\left(-\\frac{U(j)-U(i)}{kT}\\right)\\] The Metropolis solution is: \\[\\text{acc}(i\\rightarrow j) = \\min\\left(1, \\exp\\left(-\\frac{U(j)-U(i)}{kT}\\right)\\right)\\] This formula means: - If the new state has lower energy \\((U(j) &lt; U(i))\\), accept the move with certainty - If the new state has higher energy, accept with probability \\(\\exp(-(U(j)-U(i))/kT)\\) 2.6.4 The Significance of the Metropolis Approach The Metropolis algorithm has several important properties: - It samples from the correct Boltzmann distribution without requiring calculation of the partition function - It focuses computational effort on the relevant regions of configuration space (those with significant Boltzmann weight) - It works effectively in high-dimensional spaces - It only requires the ability to calculate energy differences, not absolute probabilities This method has expanded the capability to calculate thermodynamic averages for complex molecular systems. 2.7 Implementing Metropolis MC for Chemical Systems Let’s now examine how to implement the Metropolis algorithm for practical chemical simulations. 2.7.1 Basic Algorithm The basic Metropolis Monte Carlo algorithm follows these steps: Start with an initial configuration \\(\\mathbf{r}\\) Calculate its energy \\(U(\\mathbf{r})\\) Repeat many times: Propose a move \\(\\mathbf{r} \\rightarrow \\mathbf{r}&#39;\\) Calculate the energy of the new configuration \\(U(\\mathbf{r}&#39;)\\) Calculate the energy difference \\(\\Delta U = U(\\mathbf{r}&#39;) - U(\\mathbf{r})\\) If \\(\\Delta U \\leq 0\\): accept the move Else: generate a random number \\(\\xi\\) between 0 and 1 If \\(\\xi &lt; \\exp(-\\Delta U/kT)\\): accept the move Else: reject the move (keep the original configuration) If the move is accepted, update \\(\\mathbf{r} = \\mathbf{r}&#39;\\) Calculate and store properties of interest for the current configuration Calculate the average of the stored properties Here’s a simple Python implementation of the Metropolis algorithm for a 1D system: import numpy as np import matplotlib.pyplot as plt def metropolis_1d(potential_energy, initial_position, kT, step_size, n_steps): &quot;&quot;&quot; Implement the Metropolis algorithm for a 1D system. Args: potential_energy: Function that calculates the potential energy initial_position: Starting position kT: Boltzmann constant times temperature step_size: Maximum displacement in a single move n_steps: Number of Monte Carlo steps Returns: Array of positions sampled according to the Boltzmann distribution &quot;&quot;&quot; positions = np.zeros(n_steps) positions[0] = initial_position current_position = initial_position current_energy = potential_energy(current_position) accepted = 0 for i in range(1, n_steps): # Propose a move proposed_position = current_position + np.random.uniform(-step_size, step_size) proposed_energy = potential_energy(proposed_position) # Calculate energy difference delta_U = proposed_energy - current_energy # Apply Metropolis criterion if delta_U &lt;= 0 or np.random.random() &lt; np.exp(-delta_U / kT): # Accept the move current_position = proposed_position current_energy = proposed_energy accepted += 1 # Store current position positions[i] = current_position acceptance_rate = accepted / (n_steps - 1) print(f&quot;Acceptance rate: {acceptance_rate:.2f}&quot;) return positions 2.7.2 Move Proposal Mechanisms The effectiveness of Metropolis MC depends on how we propose new configurations. Common approaches include: Atomic displacements: Randomly select an atom and displace it by a small random vector. r_new(i) = r_old(i) + δr Where δr is typically drawn from a uniform or Gaussian distribution. Dihedral angle rotations: For molecules, rotate a portion of the molecule around a bond. φ_new = φ_old + δφ Where δφ is a small random angle change. Molecule translations/rotations: For multi-molecule systems, move entire molecules. 2.7.3 Step Size Adjustment The choice of step size (magnitude of δr or δφ) affects sampling efficiency: - If steps are too small, the simulation explores configuration space slowly - If steps are too large, most moves will be rejected due to high energy increases As a rule of thumb, aim for an acceptance rate of approximately 50%. During the simulation setup phase, the step size can be adjusted to achieve this target. 2.7.4 System Representation To implement Metropolis MC, we need: - A way to represent the system’s configuration (typically atomic coordinates) - A method to calculate the potential energy (force field or quantum mechanical calculation) - Appropriate boundary conditions (periodic boundaries for bulk systems) 2.7.5 Practical Considerations Equilibration period: The initial configuration may be far from typical equilibrium configurations. Therefore, we typically discard data from an initial “equilibration” phase before collecting statistics. The simulation should run until any memory of the initial state is lost. Correlation between samples: Successive configurations in a Metropolis simulation are correlated. When calculating statistical errors, we must account for this correlation, typically using block averaging methods. Convergence assessment: Multiple independent runs starting from different initial configurations can help verify convergence. If they yield consistent results, we can be more confident in our sampling. 2.8 Chemical Example: Conformational Sampling of Butane To illustrate the application of Metropolis Monte Carlo to a chemical system, let’s consider the conformational preferences of butane (C₄H₁₀). 2.8.1 The Butane Molecule Butane has three main conformations determined by the dihedral angle φ around the central C-C bond: - Anti (φ = 180°): The lowest energy conformation - Gauche+ (φ ≈ 60°): Slightly higher in energy - Gauche- (φ ≈ -60°): Symmetrically equivalent to gauche+ The potential energy as a function of this dihedral angle can be approximated by: \\[U(\\phi) = A_0 + A_1(1+\\cos\\phi) + A_2(1-\\cos2\\phi) + A_3(1+\\cos3\\phi)\\] Where the constants \\(A_0\\), \\(A_1\\), \\(A_2\\), and \\(A_3\\) are determined from either experimental data or quantum mechanical calculations. This function has minima at the anti and gauche positions, with energy barriers between them. Here’s how we might implement this for butane: import numpy as np import matplotlib.pyplot as plt def butane_potential(phi, A0=0, A1=1.522, A2=0.627, A3=3.080): &quot;&quot;&quot; Calculate the torsional potential energy of butane. Args: phi: Dihedral angle in radians A0, A1, A2, A3: Parameters for the torsional potential Returns: Potential energy in kJ/mol &quot;&quot;&quot; return A0 + A1*(1+np.cos(phi)) + A2*(1-np.cos(2*phi)) + A3*(1+np.cos(3*phi)) # Plot the potential energy function phi_values = np.linspace(-np.pi, np.pi, 1000) energy_values = [butane_potential(phi) for phi in phi_values] plt.figure(figsize=(10, 6)) plt.plot(phi_values * 180/np.pi, energy_values) plt.xlabel(&#39;Dihedral angle (degrees)&#39;) plt.ylabel(&#39;Potential energy (kJ/mol)&#39;) plt.title(&#39;Butane torsional potential&#39;) plt.grid(True) plt.axvline(x=-60, color=&#39;r&#39;, linestyle=&#39;--&#39;, label=&#39;Gauche-&#39;) plt.axvline(x=60, color=&#39;g&#39;, linestyle=&#39;--&#39;, label=&#39;Gauche+&#39;) plt.axvline(x=180, color=&#39;b&#39;, linestyle=&#39;--&#39;, label=&#39;Anti&#39;) plt.legend() plt.savefig(&#39;butane_potential.png&#39;, dpi=300) # Run Metropolis MC simulation kT = 2.5 # kJ/mol (approximately room temperature) initial_angle = 0 # radians step_size = 0.2 # radians n_steps = 100000 angles = metropolis_1d(butane_potential, initial_angle, kT, step_size, n_steps) # Plot distribution of sampled angles plt.figure(figsize=(10, 6)) plt.hist(angles * 180/np.pi, bins=50, density=True, alpha=0.7) plt.xlabel(&#39;Dihedral angle (degrees)&#39;) plt.ylabel(&#39;Probability density&#39;) plt.title(&#39;Butane conformational distribution from Metropolis MC&#39;) plt.grid(True) plt.axvline(x=-60, color=&#39;r&#39;, linestyle=&#39;--&#39;, label=&#39;Gauche-&#39;) plt.axvline(x=60, color=&#39;g&#39;, linestyle=&#39;--&#39;, label=&#39;Gauche+&#39;) plt.axvline(x=180, color=&#39;b&#39;, linestyle=&#39;--&#39;, label=&#39;Anti&#39;) plt.legend() plt.savefig(&#39;butane_distribution.png&#39;, dpi=300) Butane potential energy function Butane conformational distribution 2.8.2 Monte Carlo Simulation Approach For this system, our Monte Carlo simulation would use: State representation: The dihedral angle φ (we simplify by focusing on just this degree of freedom) Move proposal: Small random changes in the dihedral angle: φ_new = φ_old + δφ Where δφ might be drawn from a uniform distribution between -15° and +15°. Energy evaluation: Calculate U(φ_new) using the torsional potential function Acceptance criterion: Apply the Metropolis rule: acc = min(1, exp(-(U(φ_new)-U(φ_old))/kT)) Properties to calculate: We might be interested in: - The probability distribution of dihedral angles - The average end-to-end distance of the molecule - The ratio of populations in gauche vs. anti conformations 2.8.3 Expected Results and Analysis After running the simulation, we would expect to see: Dihedral angle distribution: A probability distribution with peaks at φ = 180° (anti) and φ = ±60° (gauche), with the anti peak being higher due to its lower energy. Temperature dependence: At higher temperatures, the gauche conformations become more populated due to their greater entropy (there are two gauche conformations but only one anti). Comparison with analytical results: For this simple system, we can also calculate the exact Boltzmann-weighted averages analytically and compare them with our Monte Carlo results as a validation. Physical insights: The simulation helps us understand the flexibility of alkane chains and the balance between energetic preferences (favoring anti) and entropic factors (favoring gauche). These concepts are directly relevant to understanding polymer conformations and the behavior of lipid chains in biological membranes. 2.9 Summary In this lecture, we’ve expanded our Monte Carlo toolkit to handle the complexities of chemical systems: We’ve seen why chemical systems require Boltzmann-weighted sampling rather than uniform sampling We’ve explored the challenges posed by the partition function and high dimensionality We’ve introduced Markov Chain Monte Carlo as a general approach for sampling from complex distributions We’ve derived the Metropolis algorithm from detailed balance principles We’ve examined practical aspects of implementing Monte Carlo for molecular systems We’ve applied these concepts to a concrete chemical example: butane conformations The Metropolis algorithm allows us to calculate thermodynamic averages without knowing the partition function, while focusing our computational effort on the relevant regions of configuration space. In our next lecture, we’ll take Monte Carlo methods in a new direction by incorporating time evolution. While the Metropolis algorithm is useful for equilibrium properties, Kinetic Monte Carlo methods allow us to simulate the dynamic evolution of systems over time. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
