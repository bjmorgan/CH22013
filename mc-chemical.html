<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Lecture 2 Monte Carlo Methods in Computational Chemistry | CH22013 Lecture Notes</title>
  <meta name="description" content="These are notes to accompany the 2025 CH22013 lecture course on Monte Carlo methods." />
  <meta name="generator" content="bookdown 0.42 and GitBook 2.6.7" />

  <meta property="og:title" content="Lecture 2 Monte Carlo Methods in Computational Chemistry | CH22013 Lecture Notes" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="These are notes to accompany the 2025 CH22013 lecture course on Monte Carlo methods." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Lecture 2 Monte Carlo Methods in Computational Chemistry | CH22013 Lecture Notes" />
  
  <meta name="twitter:description" content="These are notes to accompany the 2025 CH22013 lecture course on Monte Carlo methods." />
  

<meta name="author" content="Benjamin J. Morgan" />


<meta name="date" content="2025-03-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="monte-carlo-methods-in-computational-chemistry.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">CH22013 Monte Carlo Methods</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About</a></li>
<li class="chapter" data-level="1" data-path="monte-carlo-methods-in-computational-chemistry.html"><a href="monte-carlo-methods-in-computational-chemistry.html"><i class="fa fa-check"></i><b>1</b> Monte Carlo Methods in Computational Chemistry</a>
<ul>
<li class="chapter" data-level="1.1" data-path="monte-carlo-methods-in-computational-chemistry.html"><a href="monte-carlo-methods-in-computational-chemistry.html#lecture-1-introduction-to-monte-carlo-methods"><i class="fa fa-check"></i><b>1.1</b> Lecture 1: Introduction to Monte Carlo Methods</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="monte-carlo-methods-in-computational-chemistry.html"><a href="monte-carlo-methods-in-computational-chemistry.html#introduction-to-averaging-sampling"><i class="fa fa-check"></i><b>1.1.1</b> Introduction to Averaging &amp; Sampling</a></li>
<li class="chapter" data-level="1.1.2" data-path="monte-carlo-methods-in-computational-chemistry.html"><a href="monte-carlo-methods-in-computational-chemistry.html#the-solution-statistical-sampling"><i class="fa fa-check"></i><b>1.1.2</b> The Solution: Statistical Sampling</a></li>
<li class="chapter" data-level="1.1.3" data-path="monte-carlo-methods-in-computational-chemistry.html"><a href="monte-carlo-methods-in-computational-chemistry.html#two-approaches-to-sampling"><i class="fa fa-check"></i><b>1.1.3</b> Two Approaches to Sampling</a></li>
<li class="chapter" data-level="1.1.4" data-path="monte-carlo-methods-in-computational-chemistry.html"><a href="monte-carlo-methods-in-computational-chemistry.html#fundamental-monte-carlo-examples"><i class="fa fa-check"></i><b>1.1.4</b> Fundamental Monte Carlo Examples</a></li>
<li class="chapter" data-level="1.1.5" data-path="monte-carlo-methods-in-computational-chemistry.html"><a href="monte-carlo-methods-in-computational-chemistry.html#statistical-uncertainty-in-monte-carlo-methods"><i class="fa fa-check"></i><b>1.1.5</b> Statistical Uncertainty in Monte Carlo Methods</a></li>
<li class="chapter" data-level="1.1.6" data-path="monte-carlo-methods-in-computational-chemistry.html"><a href="monte-carlo-methods-in-computational-chemistry.html#summary-and-preview"><i class="fa fa-check"></i><b>1.1.6</b> Summary and Preview</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="mc-chemical.html"><a href="mc-chemical.html"><i class="fa fa-check"></i><b>2</b> Monte Carlo Methods in Computational Chemistry</a>
<ul>
<li class="chapter" data-level="2.1" data-path="mc-chemical.html"><a href="mc-chemical.html#lecture-2-monte-carlo-methods-applied-to-chemical-systems"><i class="fa fa-check"></i><b>2.1</b> Lecture 2: Monte Carlo Methods Applied to Chemical Systems</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="mc-chemical.html"><a href="mc-chemical.html#from-uniform-to-non-uniform-sampling"><i class="fa fa-check"></i><b>2.1.1</b> From Uniform to Non-Uniform Sampling</a></li>
<li class="chapter" data-level="2.1.2" data-path="mc-chemical.html"><a href="mc-chemical.html#the-partition-function-challenge"><i class="fa fa-check"></i><b>2.1.2</b> The Partition Function Challenge</a></li>
<li class="chapter" data-level="2.1.3" data-path="mc-chemical.html"><a href="mc-chemical.html#the-challenge-of-high-dimensions"><i class="fa fa-check"></i><b>2.1.3</b> The Challenge of High Dimensions</a></li>
<li class="chapter" data-level="2.1.4" data-path="mc-chemical.html"><a href="mc-chemical.html#the-solution-markov-chain-monte-carlo"><i class="fa fa-check"></i><b>2.1.4</b> The Solution: Markov Chain Monte Carlo</a></li>
<li class="chapter" data-level="2.1.5" data-path="mc-chemical.html"><a href="mc-chemical.html#what-is-a-markov-chain"><i class="fa fa-check"></i><b>2.1.5</b> What is a Markov Chain?</a></li>
<li class="chapter" data-level="2.1.6" data-path="mc-chemical.html"><a href="mc-chemical.html#the-principle-of-detailed-balance"><i class="fa fa-check"></i><b>2.1.6</b> The Principle of Detailed Balance</a></li>
<li class="chapter" data-level="2.1.7" data-path="mc-chemical.html"><a href="mc-chemical.html#the-metropolis-algorithm"><i class="fa fa-check"></i><b>2.1.7</b> The Metropolis Algorithm</a></li>
<li class="chapter" data-level="2.1.8" data-path="mc-chemical.html"><a href="mc-chemical.html#the-metropolis-monte-carlo-algorithm"><i class="fa fa-check"></i><b>2.1.8</b> The Metropolis Monte Carlo Algorithm</a></li>
<li class="chapter" data-level="2.1.9" data-path="mc-chemical.html"><a href="mc-chemical.html#the-role-of-temperature-in-monte-carlo-sampling"><i class="fa fa-check"></i><b>2.1.9</b> The Role of Temperature in Monte Carlo Sampling</a></li>
<li class="chapter" data-level="2.1.10" data-path="mc-chemical.html"><a href="mc-chemical.html#practical-considerations"><i class="fa fa-check"></i><b>2.1.10</b> Practical Considerations</a></li>
<li class="chapter" data-level="2.1.11" data-path="mc-chemical.html"><a href="mc-chemical.html#example-conformational-sampling-of-butane"><i class="fa fa-check"></i><b>2.1.11</b> Example: Conformational Sampling of Butane</a></li>
<li class="chapter" data-level="2.1.12" data-path="mc-chemical.html"><a href="mc-chemical.html#comparison-with-molecular-dynamics"><i class="fa fa-check"></i><b>2.1.12</b> Comparison with Molecular Dynamics</a></li>
<li class="chapter" data-level="2.1.13" data-path="mc-chemical.html"><a href="mc-chemical.html#summary-and-preview-1"><i class="fa fa-check"></i><b>2.1.13</b> Summary and Preview</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">CH22013 Lecture Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mc-chemical" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Lecture 2</span> Monte Carlo Methods in Computational Chemistry<a href="mc-chemical.html#mc-chemical" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="lecture-2-monte-carlo-methods-applied-to-chemical-systems" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Lecture 2: Monte Carlo Methods Applied to Chemical Systems<a href="mc-chemical.html#lecture-2-monte-carlo-methods-applied-to-chemical-systems" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="from-uniform-to-non-uniform-sampling" class="section level3 hasAnchor" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> From Uniform to Non-Uniform Sampling<a href="mc-chemical.html#from-uniform-to-non-uniform-sampling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In our previous lecture, we explored Monte Carlo methods using uniform sampling. We saw how randomly selecting points with equal probability allows us to estimate mathematical quantities like <span class="math inline">\(\pi\)</span> or evaluate definite integrals. These examples worked well because each possible sample point had the same likelihood of occurring.</p>
<p>Chemical systems, however, present a different challenge. In molecular systems, configurations are not equally probable but are distributed according to their energies.</p>
<p>The probability of observing a particular molecular configuration depends on its energy according to the Boltzmann distribution:</p>
<p><span class="math inline">\(P(\mathbf{r}) \propto \exp(-U(\mathbf{r})/kT)\)</span></p>
<p>As discussed in Lecture 1, this distribution means low-energy configurations are exponentially more probable than high-energy ones. This fundamental property governs the behavior of all molecular systems at equilibrium.</p>
<p>This non-uniform probability distribution changes how we must calculate averages. With uniform sampling, as we saw in Lecture 1, we could simply average our results:</p>
<p><span class="math display">\[\langle A \rangle \approx \frac{1}{N} \sum_{i=1}^N A(\mathbf{r}_i)\]</span></p>
<p>For chemical systems with their non-uniform probabilities, we need a weighted average:</p>
<p><span class="math display">\[\langle A \rangle \approx \frac{\sum_{i=1}^N A(\mathbf{r}_i) \times w(\mathbf{r}_i)}{\sum_{i=1}^N w(\mathbf{r}_i)}\]</span></p>
<p>The weight <span class="math inline">\(w(\mathbf{r}_i)\)</span> represents the relative probability of configuration <span class="math inline">\(\mathbf{r}_i\)</span>, which follows the Boltzmann distribution: <span class="math inline">\(w(\mathbf{r}_i) \propto \exp(-U(\mathbf{r}_i)/kT)\)</span>.</p>
<p>This presents us with a new challenge: how do we generate samples that follow the Boltzmann distribution? This is the central question we’ll address in this lecture.</p>
</div>
<div id="the-partition-function-challenge" class="section level3 hasAnchor" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> The Partition Function Challenge<a href="mc-chemical.html#the-partition-function-challenge" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In Lecture 1, we introduced the partition function <span class="math inline">\(Z\)</span> as the normalization constant in the Boltzmann distribution:</p>
<p><span class="math inline">\(P(\mathbf{r}) = \frac{\exp(-U(\mathbf{r})/kT)}{Z}\)</span></p>
<p>Where:
<span class="math inline">\(Z = \sum \exp(-U(\mathbf{r})/kT)\)</span></p>
<p>This normalization ensures that probabilities sum to 1 and provides connections to thermodynamic properties.</p>
<p>However, calculating the partition function directly creates a fundamental computational challenge. To determine <span class="math inline">\(Z\)</span> exactly, we would need to sum over all possible configurations of our system—the very sum we’re trying to avoid by using sampling methods. For most chemical systems, the number of possible configurations is astronomical. Even a small protein might have more possible conformations than there are atoms in the universe.</p>
<p>This creates a practical dilemma: to calculate the Boltzmann probabilities correctly, we need to know <span class="math inline">\(Z\)</span>, but calculating <span class="math inline">\(Z\)</span> requires evaluating all possible states. We need a method that can sample according to the Boltzmann distribution without knowing <span class="math inline">\(Z\)</span> in advance.</p>
</div>
<div id="the-challenge-of-high-dimensions" class="section level3 hasAnchor" number="2.1.3">
<h3><span class="header-section-number">2.1.3</span> The Challenge of High Dimensions<a href="mc-chemical.html#the-challenge-of-high-dimensions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Our sampling challenge is further complicated by what mathematicians call the “curse of dimensionality.” This refers to how the volume of configuration space grows exponentially with its dimension.</p>
<p>A simple example illustrates this problem mathematically. Consider the volume ratio of a hypersphere inscribed within a hypercube:</p>
<ul>
<li>In 2D: A circle within a square occupies about 79% of the square’s area</li>
<li>In 3D: A sphere within a cube occupies only about 52% of the cube’s volume</li>
<li>In 10D: The hypersphere occupies a mere 0.25% of the hypercube</li>
<li>In 100D: The ratio becomes extremely small (approximately <span class="math inline">\(10^{-70}\)</span>)</li>
</ul>
<p>For molecular systems, the dimensionality equals the degrees of freedom. Each atom has three spatial coordinates, so a system with <span class="math inline">\(N\)</span> atoms has <span class="math inline">\(3N\)</span> dimensions (minus six for overall translation and rotation of the entire molecule). Even a small protein with 100 residues easily has thousands of dimensions.</p>
<p>This high dimensionality creates a severe sampling problem. If we were to use uniform sampling in such a high-dimensional space, the vast majority of randomly generated configurations would have extremely low Boltzmann probability—typically corresponding to physically impossible structures with severe atomic overlaps.</p>
<p>The curse of dimensionality means that as system size increases, the fraction of configuration space with significant Boltzmann weight becomes vanishingly small, making uniform sampling hopelessly inefficient.</p>
</div>
<div id="the-solution-markov-chain-monte-carlo" class="section level3 hasAnchor" number="2.1.4">
<h3><span class="header-section-number">2.1.4</span> The Solution: Markov Chain Monte Carlo<a href="mc-chemical.html#the-solution-markov-chain-monte-carlo" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We’ve identified two interconnected challenges: we can’t calculate the Boltzmann distribution directly without knowing the partition function, and even if we could, uniform sampling becomes hopelessly inefficient in high dimensions. Fortunately, a class of methods known as Markov Chain Monte Carlo (MCMC) provides an elegant solution to both problems.</p>
<p>Let’s return to our example from Lecture 1 of estimating the average height of people in Britain. Imagine if instead of sampling individuals directly, we decided to use a geographical approach: randomly selecting 100m × 100m squares on a map of Britain, and measuring everyone within each selected square.</p>
<p>This approach would be extremely inefficient. Many squares would contain no people at all (in rural areas, forests, lakes), while a few squares in London or Manchester might contain hundreds of people. To get a representative sample, we would need to select an enormous number of squares.</p>
<p>A more efficient approach would be to preferentially sample areas with higher population density. Moreover, we could use the fact that population density tends to be spatially correlated—if one square has many people, neighboring squares likely do as well.</p>
<p>This is the key insight of MCMC. Instead of generating independent random samples (squares) with uniform probability, we create a “chain” of samples where each new sample is generated based on the current one. In our population example, we might start in a random square, then preferentially move to neighboring squares with higher population. Over time, we would naturally spend most of our sampling effort in densely populated areas, with occasional visits to less populated regions.</p>
<p>For molecular systems, MCMC creates a random walk through configuration space that naturally spends more time in high-probability (low-energy) regions, in proportion to their Boltzmann weight. This solves both our challenges: we sample efficiently from regions that matter most, and we never need to calculate the partition function explicitly.</p>
</div>
<div id="what-is-a-markov-chain" class="section level3 hasAnchor" number="2.1.5">
<h3><span class="header-section-number">2.1.5</span> What is a Markov Chain?<a href="mc-chemical.html#what-is-a-markov-chain" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Before diving into the full algorithm, let’s understand what makes something a “Markov chain.” A Markov chain is a sequence of states where the probability of the next state depends only on the current state, not on the sequence of states that preceded it. This “memoryless” property is known as the Markov property.</p>
<p>A simple example is a random walk where the next position depends only on the current position and some transition rules. The future evolution of the system depends only on its present state, not on how it arrived at that state.</p>
<p>Markov chains are characterized by transition probabilities—the likelihood of moving from one state to another. For a system with discrete states, these probabilities form a transition matrix. After many steps, many Markov chains converge to a “stationary distribution” where the probability of finding the system in each state no longer changes over time.</p>
<p>This stationary distribution is the key to our solution. If we design our Markov chain so that its stationary distribution matches the Boltzmann distribution, then by running the chain for a sufficient number of steps, we’ll naturally sample configurations with the correct probabilities—all without ever having to calculate the partition function.</p>
</div>
<div id="the-principle-of-detailed-balance" class="section level3 hasAnchor" number="2.1.6">
<h3><span class="header-section-number">2.1.6</span> The Principle of Detailed Balance<a href="mc-chemical.html#the-principle-of-detailed-balance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>How do we design a Markov chain that converges to the Boltzmann distribution? The answer lies in a principle called “detailed balance.”</p>
<p>At equilibrium, the overall population of each state in a Markov chain remains constant. This means that the total probability flow into a state must equal the total flow out of that state. This is called “global balance.”</p>
<p>Detailed balance is a stronger condition that ensures global balance. It states that for each pair of states, the flow from state i to state j must exactly equal the flow from j back to i:</p>
<p><span class="math inline">\(\pi(i) \times P(i \to j) = \pi(j) \times P(j \to i)\)</span></p>
<p>Here, <span class="math inline">\(\pi(i)\)</span> is the equilibrium probability of state i, and <span class="math inline">\(P(i \to j)\)</span> is the transition probability from state i to state j.</p>
<p>For chemical systems, we want <span class="math inline">\(\pi(i)\)</span> to match the Boltzmann distribution. Substituting this in:</p>
<p><span class="math inline">\(\exp(-U(i)/kT) \times P(i \to j) = \exp(-U(j)/kT) \times P(j \to i)\)</span></p>
<p>Rearranging:</p>
<p><span class="math inline">\(\frac{P(i \to j)}{P(j \to i)} = \frac{\exp(-U(j)/kT)}{\exp(-U(i)/kT)} = \exp(-(U(j)-U(i))/kT)\)</span></p>
<p>This equation gives us a constraint on the transition probabilities. Any Markov chain that satisfies this relationship will have the Boltzmann distribution as its stationary distribution. The challenge is now to design practical transitions that satisfy this constraint.</p>
</div>
<div id="the-metropolis-algorithm" class="section level3 hasAnchor" number="2.1.7">
<h3><span class="header-section-number">2.1.7</span> The Metropolis Algorithm<a href="mc-chemical.html#the-metropolis-algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In 1953, Nicholas Metropolis and colleagues published an algorithm that provides a simple and powerful way to satisfy detailed balance. Their approach has become a cornerstone of computational chemistry and physics.</p>
<p>The key insight of the Metropolis method is to design an acceptance criterion that ensures sampling from the Boltzmann distribution while satisfying detailed balance. The algorithm separates the transition probability into two parts:</p>
<p><span class="math inline">\(P(i \to j) = \alpha(i \to j) \times \text{acc}(i \to j)\)</span></p>
<p>Where <span class="math inline">\(\alpha(i \to j)\)</span> is the proposal probability and <span class="math inline">\(\text{acc}(i \to j)\)</span> is the acceptance probability.</p>
<p>The simplest approach is to use symmetric proposal probabilities where <span class="math inline">\(\alpha(i \to j) = \alpha(j \to i)\)</span>. This might involve, for example, randomly displacing an atom in any direction with equal probability. With this simplification, our detailed balance condition becomes:</p>
<p><span class="math inline">\(\frac{\text{acc}(i \to j)}{\text{acc}(j \to i)} = \exp(-(U(j)-U(i))/kT)\)</span></p>
<p>The Metropolis solution to this equation is:</p>
<p><span class="math inline">\(\text{acc}(i \to j) = \min(1, \exp(-(U(j)-U(i))/kT))\)</span></p>
<p>This elegant formula leads to a simple rule:</p>
<ul>
<li>If the proposed move decreases the energy (<span class="math inline">\(U(j) &lt; U(i)\)</span>), accept it with probability 1 (always)</li>
<li>If the proposed move increases the energy, accept it with probability <span class="math inline">\(\exp(-(U(j)-U(i))/kT)\)</span></li>
</ul>
<p>This acceptance rule makes intuitive sense. The system always accepts moves to lower energy states, just as a ball naturally rolls downhill. But it sometimes accepts moves to higher energy states, with a probability that decreases exponentially with the energy increase. This allows the system to escape local energy minima and explore the full configuration space.</p>
</div>
<div id="the-metropolis-monte-carlo-algorithm" class="section level3 hasAnchor" number="2.1.8">
<h3><span class="header-section-number">2.1.8</span> The Metropolis Monte Carlo Algorithm<a href="mc-chemical.html#the-metropolis-monte-carlo-algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s put everything together into a practical algorithm for simulating chemical systems:</p>
<ol style="list-style-type: decimal">
<li><p>Start with an initial configuration of your system.</p></li>
<li><p>Calculate the energy of this initial configuration <span class="math inline">\(U(\mathbf{r})\)</span>.</p></li>
<li><p>Propose a move to a new configuration <span class="math inline">\(\mathbf{r}&#39;\)</span>. This might involve randomly displacing an atom, rotating a dihedral angle, or some other change to the system.</p></li>
<li><p>Calculate the energy of the new configuration <span class="math inline">\(U(\mathbf{r}&#39;)\)</span>.</p></li>
<li><p>Compute the energy difference: <span class="math inline">\(\Delta U = U(\mathbf{r}&#39;) - U(\mathbf{r})\)</span>.</p></li>
<li><p>Apply the Metropolis criterion to decide whether to accept the move:</p>
<ul>
<li>If <span class="math inline">\(\Delta U \leq 0\)</span>, accept the move.</li>
<li>If <span class="math inline">\(\Delta U &gt; 0\)</span>, generate a random number <span class="math inline">\(\xi\)</span> between 0 and 1.
<ul>
<li>If <span class="math inline">\(\xi &lt; \exp(-\Delta U/kT)\)</span>, accept the move.</li>
<li>Otherwise, reject the move.</li>
</ul></li>
</ul></li>
<li><p>If the move is accepted, update your current configuration to <span class="math inline">\(\mathbf{r}&#39;\)</span>. If rejected, retain the original configuration <span class="math inline">\(\mathbf{r}\)</span>.</p></li>
<li><p>Calculate any properties of interest for the current configuration.</p></li>
<li><p>Return to step 3 and repeat for many iterations.</p></li>
<li><p>Compute the average of your calculated properties over all sampled configurations.</p></li>
</ol>
<p>This algorithm naturally generates configurations according to the Boltzmann distribution. The beauty of the approach is that we never need to calculate the partition function—the acceptance rule ensures the correct distribution without requiring normalization.</p>
</div>
<div id="the-role-of-temperature-in-monte-carlo-sampling" class="section level3 hasAnchor" number="2.1.9">
<h3><span class="header-section-number">2.1.9</span> The Role of Temperature in Monte Carlo Sampling<a href="mc-chemical.html#the-role-of-temperature-in-monte-carlo-sampling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Temperature plays a crucial role in Monte Carlo simulations, affecting both the equilibrium distribution and sampling efficiency.</p>
<p>Looking at the Metropolis acceptance criterion:</p>
<p><span class="math inline">\(\text{acc}(i \to j) = \min(1, \exp(-(U(j)-U(i))/kT))\)</span></p>
<p>Temperature appears in the denominator of the exponent. At high temperatures, the simulation readily accepts moves to higher energy states. At low temperatures, it rejects most uphill moves.</p>
<p>This creates a trade-off: low temperatures accurately reflect the system’s stable states but can trap simulations in local minima. High temperatures explore configuration space efficiently but may not emphasize physically relevant states appropriately.</p>
<p>The butane example illustrates this perfectly. At very low temperatures, a simulation starting in the anti conformation would rarely sample the gauche states. At room temperature, transitions between conformations occur regularly, allowing proper sampling. At very high temperatures, the simulation would sample all dihedral angles almost equally, failing to capture the natural preferences of the molecule.</p>
</div>
<div id="practical-considerations" class="section level3 hasAnchor" number="2.1.10">
<h3><span class="header-section-number">2.1.10</span> Practical Considerations<a href="mc-chemical.html#practical-considerations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>While the Metropolis algorithm is conceptually simple, several practical considerations affect its efficiency in real simulations.</p>
<div id="move-selection" class="section level4 hasAnchor" number="2.1.10.1">
<h4><span class="header-section-number">2.1.10.1</span> Move Selection<a href="mc-chemical.html#move-selection" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The way we propose new configurations significantly impacts sampling efficiency. Different types of moves are appropriate for different systems:</p>
<p>For atomic systems, simple displacements work well. We randomly select an atom and move it by a small amount in a random direction.</p>
<p>For molecular systems, we often need more sophisticated moves. Rotating around bonds (changing dihedral angles) is particularly effective for organic molecules. For systems with multiple molecules, we might translate or rotate entire molecules as a unit.</p>
</div>
<div id="step-size-tuning" class="section level4 hasAnchor" number="2.1.10.2">
<h4><span class="header-section-number">2.1.10.2</span> Step Size Tuning<a href="mc-chemical.html#step-size-tuning" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The magnitude of the proposed moves—the “step size”—is crucial for efficient sampling. If steps are too small, the simulation explores configuration space very slowly. If steps are too large, most moves will be rejected because they create high-energy configurations with atomic overlaps.</p>
<p>As a rule of thumb, aim for an acceptance rate around 40-50%. During the simulation setup, you can adjust the step size to achieve this target. Some advanced simulations dynamically adjust the step size as the simulation progresses.</p>
</div>
<div id="equilibration-period" class="section level4 hasAnchor" number="2.1.10.3">
<h4><span class="header-section-number">2.1.10.3</span> Equilibration Period<a href="mc-chemical.html#equilibration-period" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>When we start a simulation, our initial configuration might be far from typical equilibrium structures. To avoid biasing our results, we discard data from an initial “equilibration” phase before collecting statistics. The length of this phase depends on the system, but it should be long enough that the simulation no longer shows any memory of the initial configuration.</p>
</div>
<div id="correlated-samples" class="section level4 hasAnchor" number="2.1.10.4">
<h4><span class="header-section-number">2.1.10.4</span> Correlated Samples<a href="mc-chemical.html#correlated-samples" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Unlike the uniform Monte Carlo methods in Lecture 1, successive configurations in a Metropolis simulation are correlated. Each new configuration is generated from the previous one, so they’re not independent samples.</p>
<p>This correlation affects uncertainty estimates. When calculating statistical errors, we must account for correlation between samples. A common approach is “block averaging,” where we divide the simulation into blocks, calculate averages within each block, and then analyze the variance between block averages.</p>
</div>
</div>
<div id="example-conformational-sampling-of-butane" class="section level3 hasAnchor" number="2.1.11">
<h3><span class="header-section-number">2.1.11</span> Example: Conformational Sampling of Butane<a href="mc-chemical.html#example-conformational-sampling-of-butane" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s apply the Metropolis method to a simple but illustrative chemical example: the conformational preferences of butane (C<sub>4</sub>H<sub>10</sub>).</p>
<p>Butane serves as an excellent model system for studying conformational energetics. The rotation around its central carbon-carbon bond gives rise to three main conformations: the “anti” (φ = 180°) and two equivalent “gauche” conformations (φ ≈ ±60°).</p>
<p>The potential energy as a function of this dihedral angle can be approximated by:</p>
<p><span class="math inline">\(U(\phi) = A_0 + A_1(1+\cos\phi) + A_2(1-\cos2\phi) + A_3(1+\cos3\phi)\)</span></p>
<p>For this system, a simple Metropolis Monte Carlo simulation would involve:
1. Starting with an initial dihedral angle, perhaps 180° (anti)
2. Proposing small random changes to this angle
3. Accepting or rejecting according to the Metropolis criterion
4. Recording the dihedral angles visited during the simulation</p>
<p>After sufficient sampling, a histogram of dihedral angles would show peaks at 180° (anti) and ±60° (gauche). This illustrates an important concept in chemical systems: the balance between energy and entropy. The anti conformation is energetically favored, but there are two equivalent gauche conformations, creating an entropic preference for the gauche state.</p>
</div>
<div id="comparison-with-molecular-dynamics" class="section level3 hasAnchor" number="2.1.12">
<h3><span class="header-section-number">2.1.12</span> Comparison with Molecular Dynamics<a href="mc-chemical.html#comparison-with-molecular-dynamics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Having examined Metropolis Monte Carlo, it’s instructive to compare it with Molecular Dynamics (MD) approaches. Each method has distinct advantages for different applications:</p>
<p><strong>Molecular Dynamics:</strong>
- Follows physically realistic trajectories based on Newton’s equations
- Provides time-dependent properties and kinetic information
- Requires force calculations (energy derivatives)
- Limited by time step constraints
- May struggle with rare events separated by high energy barriers</p>
<p><strong>Metropolis Monte Carlo:</strong>
- Makes non-physical transitions between states
- Directly samples from the Boltzmann distribution
- Requires only energy differences, not forces
- Has no time step constraints
- Cannot provide dynamical information in its standard form
- Often more efficient for sampling across energy barriers</p>
<p>While the Metropolis algorithm focuses on equilibrium properties and lacks time information, specialized Monte Carlo variants (such as Kinetic Monte Carlo) can model time evolution and dynamical processes. This makes the Monte Carlo approach more versatile than it might initially appear.</p>
<p><strong>When to Choose Which Method:</strong></p>
<p>MD is often preferable when:
- Dynamical information (time-correlation functions, diffusion rates) is needed
- The system’s kinetic behavior is of primary interest
- Realistic trajectories are important for understanding mechanism</p>
<p>MC is often preferable when:
- Only equilibrium properties are of interest
- The system has high energy barriers that trap MD in metastable states
- The system has discrete degrees of freedom
- Force calculations are expensive or difficult
- Specialized sampling of specific degrees of freedom is needed</p>
<p>In practice, many computational studies use both approaches, sometimes in combination. Hybrid Monte Carlo methods incorporate elements of both MD and MC to leverage their complementary strengths.</p>
</div>
<div id="summary-and-preview-1" class="section level3 hasAnchor" number="2.1.13">
<h3><span class="header-section-number">2.1.13</span> Summary and Preview<a href="mc-chemical.html#summary-and-preview-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this lecture, we’ve expanded our Monte Carlo toolkit to address the challenges of chemical systems. We’ve seen why the Boltzmann distribution requires specialized sampling techniques, and how the Metropolis algorithm provides an elegant solution through Markov Chain Monte Carlo.</p>
<p>The key concepts we’ve covered include:</p>
<ul>
<li>The distinction between uniform and Boltzmann-weighted sampling</li>
<li>The challenges posed by the partition function and high dimensionality</li>
<li>How Markov chains can generate samples from a target distribution</li>
<li>The principle of detailed balance and its role in ensuring correct sampling</li>
<li>The Metropolis acceptance criterion and its implementation</li>
<li>Practical considerations for effective Monte Carlo simulations</li>
<li>Comparison of Monte Carlo with Molecular Dynamics approaches</li>
</ul>
<p>In our next lecture, we’ll take Monte Carlo in a new direction by incorporating time evolution. While the Metropolis algorithm focuses on equilibrium properties, Kinetic Monte Carlo methods allow us to simulate the dynamic evolution of systems over time. This will enable us to study processes like chemical reactions and diffusion using a Monte Carlo framework, opening the door to phenomena occurring on longer timescales than are accessible with traditional molecular dynamics.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="monte-carlo-methods-in-computational-chemistry.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/bjmorgan/CH22013/main/lecture_02/lecture_02.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section"
  },
  "mathjax": {
    "extensions": "mhchem.js"
  },
  "mathjax_config": {
    "TeX": {
      "Macros": {
        "conc": [
          "[\mathrm{#1}]",
          1
        ],
        "diffc": [
          "\frac{\mathrm{d}\conc{#1}}{\mathrm{d}t}",
          1
        ]
      }
    }
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
